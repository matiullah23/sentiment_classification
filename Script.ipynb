{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\600846\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nl.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass path as train and test folder\n",
    "def main():\n",
    "    # print command line arguments\n",
    "    train= get_data(sys.argv[1])\n",
    "    test= get_data(sys.argv[2])\n",
    "\n",
    "    clean_train_stem,clean_train_nostem= clean(train)\n",
    "    clean_test_stem, clean_test_nostem= clean(test)\n",
    "    \n",
    "    #Binary and Freq representations for stemmed and nonstemmed data\n",
    "    trainvector_stem_bin,trainvector_stem_freq,stem_vocab= getrep(clean_train_stem)\n",
    "    trainvector_nostem_bin,trainvector_nostem_freq, nostem_vocab = getrep(clean_train_nostem)\n",
    "    \n",
    "     #calling naive bayes and logistic regression 4 times each to train on the 4 data vectors above\n",
    "    #Naive Bayes\n",
    "    result1 = Naivebayes(clean_train_stem, [0,1], stem_vocab,trainvector_stem_bin)\n",
    "    result2 = Naivebayes(clean_train_stem, [0,1], stem_vocab,trainvector_stem_freq)\n",
    "    result3 = Naivebayes(clean_train_nostem, [0,1], nostem_vocab,trainvector_nostem_bin)\n",
    "    result4 = Naivebayes(clean_train_nostem, [0,1], nostem_vocab,trainvector_nostem_freq)\n",
    "    \n",
    "    #Logistic Regression\n",
    "    result5 = logisticreg(clean_train_stem, [0,1], freqVector,bigdoc)\n",
    "    result6 = logisticreg(clean_train_stem, [0,1], freqVector,bigdoc)\n",
    "    result7 = logisticreg(clean_train_stem, [0,1], freqVector,bigdoc)\n",
    "    result8 = logisticreg(clean_train_stem, [0,1], freqVector,bigdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-681efee17588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#calling test for each of the  models created: Print the accuracy and confusion matrix for each of them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-f5ee379a60cb>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# print command line arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "#calling test for each of the  models created: Print the accuracy and confusion matrix for each of them\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    my_dir_path_pos = path + '/positive'\n",
    "    # create list to store text\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    # loop through files and append text to list\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "            results[\"text\"].append(file_open.read())\n",
    "\n",
    "    # read the list in as a dataframe\n",
    "    df_pos = pd.DataFrame(results)\n",
    "\n",
    "    # take a look at dataframe\n",
    "    df_pos.head()\n",
    "\n",
    "    #set directory path\n",
    "    my_dir_path_neg = path + '/negative'\n",
    "\n",
    "    # create list to store text\n",
    "    results_neg = defaultdict(list)\n",
    "\n",
    "    # loop through files and append text to list\n",
    "    for file in Path(my_dir_path_neg).iterdir():\n",
    "        with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "            results_neg[\"text\"].append(file_open.read())\n",
    "    # read the list in as a dataframe\n",
    "    df_neg = pd.DataFrame(results_neg)\n",
    "    df_neg.head()\n",
    "\n",
    "    #add sentiment to both datasets and then combine them for test data 1 for positive and 0 for negative\n",
    "    df_pos['Sentiment']=1\n",
    "    df_neg['Sentiment']=0\n",
    "    frames = [df_pos, df_neg]\n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    # increase column width to see more of the tweets\n",
    "    pd.set_option('max_colwidth', 140)\n",
    "\n",
    "    # reshuffle the tweets to see both pos and neg in random order\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # explore top 5 rows\n",
    "    df.head(5)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "\n",
    "    # Remove any markup tags (HTML), all the mentions of handles(starts with '@') and '#' character\n",
    "    def cleantweettext(raw_html):\n",
    "        pattern = re.compile('<.*?>')\n",
    "        cleantext = re.sub(pattern, '', raw_html)\n",
    "        cleantext = \" \".join(filter(lambda x:x[0]!='@', cleantext.split()))\n",
    "        cleantext = cleantext.replace('#', '')\n",
    "        return cleantext\n",
    "\n",
    "    def removeat(text):\n",
    "        atlist=[]\n",
    "        for word in text:\n",
    "            pattern = re.compile('^@')\n",
    "            if re.match(pattern,word):\n",
    "                #cleantext1 = re.sub(pattern, word[1:], word)\n",
    "                atlist.append(word[1:])\n",
    "            else:\n",
    "                atlist.append(word)\n",
    "        return atlist\n",
    "\n",
    "    def tolower(text):\n",
    "        lowerlist=[]\n",
    "        for word in text:\n",
    "            pattern = re.compile('[A-Z][a-z]+')\n",
    "            if re.match(pattern,word):\n",
    "                cleantext1 = re.sub(pattern, word.lower(), word)\n",
    "                lowerlist.append(cleantext1)\n",
    "            else:\n",
    "                lowerlist.append(word)\n",
    "        return lowerlist\n",
    "\n",
    "    cleantweet= []\n",
    "    for doc in df.text:\n",
    "        cleantweet.append(cleantweettext(doc))\n",
    "\n",
    "\n",
    "    tokentweet=[]\n",
    "    df.text= cleantweet\n",
    "    for doc in df.text:\n",
    "        tokentweet.append(TweetTokenizer().tokenize(doc))\n",
    "    df.text= tokentweet\n",
    "\n",
    "    removeattweet=[]\n",
    "    for doc in df.text:\n",
    "        removeattweet.append(removeat(doc))\n",
    "    df.text =removeattweet\n",
    "\n",
    "    lowertweet=[]\n",
    "    for doc in df.text:\n",
    "        lowertweet.append(tolower(doc))\n",
    "    df.text = lowertweet\n",
    "\n",
    "    tweets=[]\n",
    "    for x in df.text:\n",
    "        tweet = ''\n",
    "        for word in x:\n",
    "            tweet += word+' '\n",
    "        tweets.append(word_tokenize(tweet))\n",
    "    df.text= tweets\n",
    "\n",
    "    #stemming\n",
    "    stemtweets=[]\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "    #ps= PorterStemmer()\n",
    "    for x in df.text:\n",
    "        stemtweet=''\n",
    "        for word in x:\n",
    "            stemtweet=stemtweet+stemmer.stem(word)+' '\n",
    "        stemtweets.append(word_tokenize(stemtweet))\n",
    "    df['stemmed']=stemtweets\n",
    "\n",
    "    ### Finalize both the stemmed and unstemmed dataframes\n",
    "    df_unstemmed = df.drop(['stemmed'], axis=1)\n",
    "    df_unstemmed.head()\n",
    "\n",
    "    # create a df with stemmed text\n",
    "    df_stemmed = df.drop(['text'], axis=1)\n",
    "\n",
    "    return [df_stemmed,df_unstemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-cbffc6ad82cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-cbffc6ad82cd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def getrep()\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def getrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize count vectorizer\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def InitializeVectorization(text, kind):\n",
    "    if kind == 'binary':\n",
    "        vectorizer = CountVectorizer(binary = True, analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "    return vectorizer\n",
    "    \n",
    "def InitializeVocab(vectorizer, text):\n",
    "    vectorizer.fit(text)\n",
    "    freqVocab = vectorizer.vocabulary_\n",
    "    train_vector = vectorizer.transform(text)\n",
    "    len(freqVocab)\n",
    "    return freqVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorizationProc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a705e25c8aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Frequency - No stemming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorizationProc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfreqVocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInitializeVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VectorizationProc' is not defined"
     ]
    }
   ],
   "source": [
    "# Frequency - No stemming\n",
    "\n",
    "vectorizer = VectorizationProc(df['text'], 'binary')\n",
    "freqVocab = InitializeVocab(vectorizer, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-da903cf122c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#1.Transform pos and neg tweets into seprate vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_pos_vector1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain_neg_vector1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Create bigdoc that contains words in V, their corresponding frequencies for each class\n",
    "\n",
    "#1.Transform pos and neg tweets into seprate vectors\n",
    "train_pos_vector1 = vectorizer.transform(df[df['Sentiment']==1]['text'])\n",
    "train_neg_vector1 = vectorizer.transform(df[df['Sentiment']==0]['text'])\n",
    "\n",
    "#2. column sum of vectors(word per column)\n",
    "sum_pos = train_pos_vector1.sum(axis = 0)\n",
    "sum_neg = train_neg_vector1.sum(axis = 0)\n",
    "\n",
    "#3. Initialize bigdoc as a dataframe\n",
    "bigdoc = pd.DataFrame(index = list(set(freqVocab.keys())), columns = ['pos', 'neg'])\n",
    "\n",
    "#4. get the corresponding frequency from the above matrx and set it to bigdoc\n",
    "for word in freqVocab.keys():\n",
    "    index = freqVocab.get(word)\n",
    "    bigdoc.at[word, 'pos'] = sum_pos[:, index].item()\n",
    "    bigdoc.at[word, 'neg'] = sum_neg[:, index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naivebayes(data,category,vector,bigvec):\n",
    "    logprob = bigvec.copy()\n",
    "    priors = []\n",
    "    for cat in category:\n",
    "        ndoc= len(data)\n",
    "        nc= len(data[data['Sentiment']== cat])\n",
    "        prior = nc/ndoc\n",
    "        priors.append(prior)\n",
    "        if cat == 0:\n",
    "            colname = 'neg'\n",
    "        else:\n",
    "            colname = 'pos'\n",
    "        denominator = bigvec[colname].sum() + len(bigvec) #denominator for likelihood\n",
    "        logprob[colname] = bigvec[colname].apply(lambda x:math.log((x+1)/denominator)) #likelihood\n",
    "    return [logprob,priors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-ebe18b1f75f2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-ebe18b1f75f2>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    sum[c]  logprior[c]\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def TestNaiveBayes(testdoc, logprior, loglikelihood, category, V):\n",
    "\n",
    "    for cat in category:\n",
    "        sum[c]  logprior[c]\n",
    "        for each position i in testdoc\n",
    "        word testdoc[i]\n",
    "        if word 2 V\n",
    "        sum[c] = sum[c]+ loglikelihood[word,c]\n",
    "\n",
    "    return argmaxc sum[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticreg():\n",
    "\n",
    "    #logistic regression\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    #initialize coefficients to zero\n",
    "\n",
    "    def compute_cost(X, y, theta):\n",
    "        m = len(y)\n",
    "        h = sigmoid(X @ theta)\n",
    "        epsilon = 1e-5\n",
    "        cost = (1/m)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(X, y, params, learning_rate, iterations):\n",
    "        m = len(y)\n",
    "        cost_history = np.zeros((iterations,1))\n",
    "\n",
    "        for i in range(iterations):\n",
    "            params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y)) \n",
    "            cost_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "        return (cost_history, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
