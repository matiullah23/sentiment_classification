{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the txt files into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hari\n",
      "[nltk_data]     Ravella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\Hari Ravella\\\\Downloads\\\\sentiment_classification-master\")\n",
    "#set directory path\n",
    "my_dir_path = \"tweet/train/positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to store text\n",
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through files and append text to list\n",
    "for file in Path(my_dir_path).iterdir():\n",
    "    with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "        results[\"text\"].append(file_open.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the list in as a dataframe\n",
    "df_pos = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I would appreciate that.  Thank you.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USAirways thank you very much.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JetBlue I'm all set. About to fly. Not bad for a first date with a giant metal bird machine. She even brought snacks.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir I got a flight at 11:55am on Thursday but looking for something tomorrow anything available?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@AmericanAir you're my early frontrunner for best airline! #oscars2016\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       text\n",
       "0                                                                      @SouthwestAir I would appreciate that.  Thank you.\\n\n",
       "1                                                                                         @USAirways thank you very much.\\n\n",
       "2  @JetBlue I'm all set. About to fly. Not bad for a first date with a giant metal bird machine. She even brought snacks.\\n\n",
       "3              @SouthwestAir I got a flight at 11:55am on Thursday but looking for something tomorrow anything available?\\n\n",
       "4                                                  @AmericanAir you're my early frontrunner for best airline! #oscars2016\\n"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at dataframe\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united Really....you charge me $25 to check a bag and then you put it on a different flight....still Don't have my bag!!!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.@JetBlue thanks for making an effort. Credit where credit is due: flight 795 delayed 5 hours instead of 8 hours. #fwiw #loweredexpectat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@united plz don't advertise wifi if it's not gonna work thanks #worstflightever\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir - 800 is not int'l friendly\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USAirways thanks for a subpar travel experience and it's not even over yet #stepitup\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text\n",
       "0                 @united Really....you charge me $25 to check a bag and then you put it on a different flight....still Don't have my bag!!!\\n\n",
       "1  .@JetBlue thanks for making an effort. Credit where credit is due: flight 795 delayed 5 hours instead of 8 hours. #fwiw #loweredexpectat...\n",
       "2                                                            @united plz don't advertise wifi if it's not gonna work thanks #worstflightever\\n\n",
       "3                                                                                                  @SouthwestAir - 800 is not int'l friendly\\n\n",
       "4                                                      @USAirways thanks for a subpar travel experience and it's not even over yet #stepitup\\n"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set directory path\n",
    "my_dir_path_neg = \"tweet/train/negative\"\n",
    "\n",
    "# create list to store text\n",
    "results_neg = defaultdict(list)\n",
    "\n",
    "# loop through files and append text to list\n",
    "for file in Path(my_dir_path_neg).iterdir():\n",
    "    with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "        results_neg[\"text\"].append(file_open.read())\n",
    "        \n",
    "# read the list in as a dataframe\n",
    "df_neg = pd.DataFrame(results_neg)\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sentiment to both datasets and then combine them for test data 1 for positive and 0 for negative\n",
    "df_pos['Sentiment']=1\n",
    "df_neg['Sentiment']=0\n",
    "frames = [df_pos, df_neg]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4181, 2)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united They finally gave in a let him on. After they threatened to send him back to Vegas on coach. Thnx.\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@SouthwestAir I got it added thank you! :)\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@AmericanAir lost my cats missed their flights kept them crated 30 hrs for a  would-be 5 hr trip. You'll never touch my pets again.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@united you already have vomit so you are halfway there\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united - after having to now TAG MY OWN bags at the airport I was hoping they would actually arrive WITH me - here's hoping they arrive\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text  \\\n",
       "0                                @united They finally gave in a let him on. After they threatened to send him back to Vegas on coach. Thnx.\\n   \n",
       "1                                                                                                @SouthwestAir I got it added thank you! :)\\n   \n",
       "2       @AmericanAir lost my cats missed their flights kept them crated 30 hrs for a  would-be 5 hr trip. You'll never touch my pets again.\\n   \n",
       "3                                                                                   @united you already have vomit so you are halfway there\\n   \n",
       "4  @united - after having to now TAG MY OWN bags at the airport I was hoping they would actually arrive WITH me - here's hoping they arrive\\n   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increase column width to see more of the tweets\n",
    "pd.set_option('max_colwidth', 140)\n",
    "\n",
    "# reshuffle the tweets to see both pos and neg in random order\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# explore top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any markup tags (HTML), all the mentions of handles(starts with '@') and '#' character\n",
    "def cleantweettext(raw_html):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    cleantext = re.sub(pattern, '', raw_html)\n",
    "    cleantext = \" \".join(filter(lambda x:x[0]!='@', cleantext.split()))\n",
    "    cleantext = cleantext.replace('#', '')\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeat(text):\n",
    "    atlist=[]\n",
    "    for word in text:\n",
    "        pattern = re.compile('^@')\n",
    "        if re.match(pattern,word):\n",
    "            #cleantext1 = re.sub(pattern, word[1:], word)\n",
    "            atlist.append(word[1:])\n",
    "        else:\n",
    "            atlist.append(word)\n",
    "    return atlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolower(text):\n",
    "    lowerlist=[]\n",
    "    for word in text:\n",
    "        pattern = re.compile('[A-Z][a-z]+')\n",
    "        if re.match(pattern,word):\n",
    "            cleantext1 = re.sub(pattern, word.lower(), word)\n",
    "            lowerlist.append(cleantext1)\n",
    "        else:\n",
    "            lowerlist.append(word)\n",
    "    return lowerlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantweet= []\n",
    "for doc in df.text:\n",
    "    cleantweet.append(cleantweettext(doc))\n",
    "\n",
    "\n",
    "tokentweet=[]\n",
    "df.text= cleantweet\n",
    "for doc in df.text:\n",
    "    tokentweet.append(TweetTokenizer().tokenize(doc))\n",
    "    \n",
    "df.text= tokentweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeattweet=[]\n",
    "for doc in df.text:\n",
    "    removeattweet.append(removeat(doc))\n",
    "df.text =removeattweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowertweet=[]\n",
    "for doc in df.text:\n",
    "    lowertweet.append(tolower(doc))\n",
    "df.text = lowertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "for x in df.text:\n",
    "    tweet = ''\n",
    "    for word in x:\n",
    "        tweet += word+' '\n",
    "    tweets.append(word_tokenize(tweet))\n",
    "df.text= tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "stemtweets=[]\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "#ps= PorterStemmer()\n",
    "for x in df.text:\n",
    "    stemtweet=''\n",
    "    for word in x:\n",
    "        stemtweet=stemtweet+stemmer.stem(word)+' '\n",
    "    stemtweets.append(word_tokenize(stemtweet))\n",
    "df['stemmed']=stemtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[they, finally, gave, in, a, let, him, on, ., after, they, threatened, to, send, him, back, to, vegas, on, coach, ., thnx, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, got, it, added, thank, you, !, :, )]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[lost, my, cats, missed, their, flights, kept, them, crated, 30, hrs, for, a, would-be, 5, hr, trip, ., you'll, 'll, never, touch, my, p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[you, already, have, vomit, so, you, are, halfway, there]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-, after, having, to, now, TAG, MY, OWN, bags, at, the, airport, I, was, hoping, they, would, actually, arrive, WITH, me, -, here, 's, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "0                [they, finally, gave, in, a, let, him, on, ., after, they, threatened, to, send, him, back, to, vegas, on, coach, ., thnx, .]   \n",
       "1                                                                                                     [I, got, it, added, thank, you, !, :, )]   \n",
       "2  [lost, my, cats, missed, their, flights, kept, them, crated, 30, hrs, for, a, would-be, 5, hr, trip, ., you'll, 'll, never, touch, my, p...   \n",
       "3                                                                                    [you, already, have, vomit, so, you, are, halfway, there]   \n",
       "4  [-, after, having, to, now, TAG, MY, OWN, bags, at, the, airport, I, was, hoping, they, would, actually, arrive, WITH, me, -, here, 's, ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Finalize both the stemmed and unstemmed dataframes\n",
    "df_unstemmed = df.drop(['stemmed'], axis=1)\n",
    "df_unstemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[they, final, gave, in, a, let, him, on, ., after, they, threaten, to, send, him, back, to, vega, on, coach, ., thnx, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, got, it, ad, thank, you, !, :, )]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[lost, my, cat, miss, their, flight, kept, them, crate, 30, hrs, for, a, would-b, 5, hr, trip, ., you, 'll, ll, never, touch, my, pet, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[you, alreadi, have, vomit, so, you, are, halfway, there]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[-, after, have, to, now, tag, my, own, bag, at, the, airport, i, was, hope, they, would, actual, arriv, with, me, -, here, 's, hope, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  \\\n",
       "0          1   \n",
       "1          1   \n",
       "2          0   \n",
       "3          0   \n",
       "4          0   \n",
       "\n",
       "                                                                                                                                       stemmed  \n",
       "0                     [they, final, gave, in, a, let, him, on, ., after, they, threaten, to, send, him, back, to, vega, on, coach, ., thnx, .]  \n",
       "1                                                                                                        [i, got, it, ad, thank, you, !, :, )]  \n",
       "2  [lost, my, cat, miss, their, flight, kept, them, crate, 30, hrs, for, a, would-b, 5, hr, trip, ., you, 'll, ll, never, touch, my, pet, a...  \n",
       "3                                                                                    [you, alreadi, have, vomit, so, you, are, halfway, there]  \n",
       "4  [-, after, have, to, now, tag, my, own, bag, at, the, airport, i, was, hope, they, would, actual, arriv, with, me, -, here, 's, hope, th...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a df with stemmed text\n",
    "df_stemmed = df.drop(['text'], axis=1)\n",
    "df_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate count vectorizer\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def InitializeVectorization(text, kind):\n",
    "    if kind == 'binary':\n",
    "        vectorizer = CountVectorizer(binary = True, analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "    return vectorizer\n",
    "    \n",
    "def InitializeVocab(vectorizer, text):\n",
    "    vectorizer.fit(text)\n",
    "    freqVocab = vectorizer.vocabulary_\n",
    "    train_vector = vectorizer.transform(text)\n",
    "    len(freqVocab)\n",
    "    return freqVocab   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency - No stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = VectorizationProc(df['text'], 'binary')\n",
    "freqVocab = InitializeVocab(vectorizer, df['text'])\n",
    "\n",
    "#Create bigdoc that contains words in V, their corresponding frequencies for each class\n",
    "#1.Transform pos and neg tweets into seprate vectors\n",
    "train_pos_vector1 = vectorizer.transform(df[df['Sentiment']==1]['text'])\n",
    "train_neg_vector1 = vectorizer.transform(df[df['Sentiment']==0]['text'])\n",
    "\n",
    "#2. column sum of vectors(word per column)\n",
    "sum_pos = train_pos_vector1.sum(axis = 0)\n",
    "sum_neg = train_neg_vector1.sum(axis = 0)\n",
    "\n",
    "#3. Initialize bigdoc as a dataframe\n",
    "bigdoc = pd.DataFrame(index = list(set(freqVocab.keys())), columns = ['pos', 'neg'])\n",
    "\n",
    "#4. get the corresponding frequency from the above matrx and set it to bigdoc\n",
    "for word in freqVocab.keys():\n",
    "    index = freqVocab.get(word)\n",
    "    bigdoc.at[word, 'pos'] = sum_pos[:, index].item()\n",
    "    bigdoc.at[word, 'neg'] = sum_neg[:, index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>please</th>\n",
       "      <td>19</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b40</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lights</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thur</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hover</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabo</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hates</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>except</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos neg\n",
       "please  19  97\n",
       "b40      0   1\n",
       "lights   0   2\n",
       "thur     0   1\n",
       "hover    0   1\n",
       "cabo     1   0\n",
       "hates    0   2\n",
       "model    0   4\n",
       "except   0   7\n",
       "full     3  21"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdoc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Naivebayes(data,category,vector,bigvec):\n",
    "    logprob = bigvec.copy()\n",
    "    priors = []\n",
    "    for cat in category:\n",
    "        ndoc= len(data)\n",
    "        nc= len(data[data['Sentiment']== cat])\n",
    "        prior = nc/ndoc\n",
    "        print(prior)    \n",
    "        priors.append(prior)\n",
    "        \n",
    "        if cat == 0:\n",
    "            colname = 'neg'\n",
    "        else:\n",
    "            colname = 'pos'\n",
    "        \n",
    "        denominator = bigvec[colname].sum() + len(bigvec) #denominator for likelihood\n",
    "        logprob[colname] = bigvec[colname].apply(lambda x:math.log((x+1)/denominator)) #likelihood\n",
    "           \n",
    "    return [logprob,priors]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7175316909830184\n",
      "0.28246830901698156\n"
     ]
    }
   ],
   "source": [
    "result = Naivebayes(df, [0,1], freqVector,bigdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestNaiveBayes(testdoc, logprior, loglikelihood, category, V) returns best c\n",
    "for each class c 2 C\n",
    "sum[c]  logprior[c]\n",
    "for each position i in testdoc\n",
    "word testdoc[i]\n",
    "if word 2 V\n",
    "sum[c] sum[c]+ loglikelihood[word,c]\n",
    "return argmaxc sum[c]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
