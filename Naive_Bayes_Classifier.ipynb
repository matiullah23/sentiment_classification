{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the txt files into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hari\n",
      "[nltk_data]     Ravella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\Hari Ravella\\\\Downloads\\\\sentiment_classification-master\")\n",
    "#set directory path\n",
    "my_dir_path = \"tweet/train/positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to store text\n",
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through files and append text to list\n",
    "for file in Path(my_dir_path).iterdir():\n",
    "    with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "        results[\"text\"].append(file_open.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the list in as a dataframe\n",
    "df_pos = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I would appreciate that.  Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USAirways thank you very much.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JetBlue I'm all set. About to fly. Not bad fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir I got a flight at 11:55am on Thu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@AmericanAir you're my early frontrunner for b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @SouthwestAir I would appreciate that.  Thank ...\n",
       "1                  @USAirways thank you very much.\\n\n",
       "2  @JetBlue I'm all set. About to fly. Not bad fo...\n",
       "3  @SouthwestAir I got a flight at 11:55am on Thu...\n",
       "4  @AmericanAir you're my early frontrunner for b..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at dataframe\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united Really....you charge me $25 to check a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.@JetBlue thanks for making an effort. Credit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@united plz don't advertise wifi if it's not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir - 800 is not int'l friendly\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USAirways thanks for a subpar travel experien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @united Really....you charge me $25 to check a...\n",
       "1  .@JetBlue thanks for making an effort. Credit ...\n",
       "2  @united plz don't advertise wifi if it's not g...\n",
       "3        @SouthwestAir - 800 is not int'l friendly\\n\n",
       "4  @USAirways thanks for a subpar travel experien..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set directory path\n",
    "my_dir_path_neg = \"tweet/train/negative\"\n",
    "\n",
    "# create list to store text\n",
    "results_neg = defaultdict(list)\n",
    "\n",
    "# loop through files and append text to list\n",
    "for file in Path(my_dir_path_neg).iterdir():\n",
    "    with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "        results_neg[\"text\"].append(file_open.read())\n",
    "        \n",
    "# read the list in as a dataframe\n",
    "df_neg = pd.DataFrame(results_neg)\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sentiment to both datasets and then combine them for test data 1 for positive and 0 for negative\n",
    "df_pos['Sentiment']=1\n",
    "df_neg['Sentiment']=0\n",
    "frames = [df_pos, df_neg]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4181, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways customer service at its best! Rachel S.  took great care of us at the PHX airport. http://t.co/HG7vEqhGHy\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@united counter agents at RDU deserve a medal. #thankyou\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SouthwestAir is that the same reliable system couldn't find my info and then said it refund my credit card ?\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AmericanAir i got a new reservation for tomorrow. Thanks!\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USAirways @nm4agoodlife      5 hours on hold and no answer . Guess the synergy of a merger was really planned out\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     text  \\\n",
       "0  @USAirways customer service at its best! Rachel S.  took great care of us at the PHX airport. http://t.co/HG7vEqhGHy\\n   \n",
       "1                                                              @united counter agents at RDU deserve a medal. #thankyou\\n   \n",
       "2         @SouthwestAir is that the same reliable system couldn't find my info and then said it refund my credit card ?\\n   \n",
       "3                                                            @AmericanAir i got a new reservation for tomorrow. Thanks!\\n   \n",
       "4    @USAirways @nm4agoodlife      5 hours on hold and no answer . Guess the synergy of a merger was really planned out\\n   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increase column width to see more of the tweets\n",
    "pd.set_option('max_colwidth', 140)\n",
    "\n",
    "# reshuffle the tweets to see both pos and neg in random order\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# explore top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any markup tags (HTML), all the mentions of handles(starts with '@') and '#' character\n",
    "def cleantweettext(raw_html):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    cleantext = re.sub(pattern, '', raw_html)\n",
    "    cleantext = \" \".join(filter(lambda x:x[0]!='@', cleantext.split()))\n",
    "    cleantext = cleantext.replace('#', '')\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeat(text):\n",
    "    atlist=[]\n",
    "    for word in text:\n",
    "        pattern = re.compile('^@')\n",
    "        if re.match(pattern,word):\n",
    "            #cleantext1 = re.sub(pattern, word[1:], word)\n",
    "            atlist.append(word[1:])\n",
    "        else:\n",
    "            atlist.append(word)\n",
    "    return atlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolower(text):\n",
    "    lowerlist=[]\n",
    "    for word in text:\n",
    "        pattern = re.compile('[A-Z][a-z]+')\n",
    "        if re.match(pattern,word):\n",
    "            cleantext1 = re.sub(pattern, word.lower(), word)\n",
    "            lowerlist.append(cleantext1)\n",
    "        else:\n",
    "            lowerlist.append(word)\n",
    "    return lowerlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantweet= []\n",
    "for doc in df.text:\n",
    "    cleantweet.append(cleantweettext(doc))\n",
    "\n",
    "\n",
    "tokentweet=[]\n",
    "df.text= cleantweet\n",
    "for doc in df.text:\n",
    "    tokentweet.append(TweetTokenizer().tokenize(doc))\n",
    "    \n",
    "df.text= tokentweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeattweet=[]\n",
    "for doc in df.text:\n",
    "    removeattweet.append(removeat(doc))\n",
    "df.text =removeattweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'same',\n",
       " 'reliable',\n",
       " 'system',\n",
       " \"couldn't\",\n",
       " 'find',\n",
       " 'my',\n",
       " 'info',\n",
       " 'and',\n",
       " 'then',\n",
       " 'said',\n",
       " 'it',\n",
       " 'refund',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'card',\n",
       " '?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeattweet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowertweet=[]\n",
    "for doc in df.text:\n",
    "    lowertweet.append(tolower(doc))\n",
    "df.text = lowertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'same',\n",
       " 'reliable',\n",
       " 'system',\n",
       " \"couldn't\",\n",
       " 'find',\n",
       " 'my',\n",
       " 'info',\n",
       " 'and',\n",
       " 'then',\n",
       " 'said',\n",
       " 'it',\n",
       " 'refund',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'card',\n",
       " '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowertweet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "for x in df.text:\n",
    "    tweet = ''\n",
    "    for word in x:\n",
    "        tweet += word+' '\n",
    "    tweets.append(word_tokenize(tweet))\n",
    "df.text= tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[got, it, ., thanks, the, quick, reply, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, ’, ve, filled, out, the, form, twice, ., no, email, ., I, have, a, lost, item, code, ., can, you, verify, it, was, received, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bluemanity, loves, this, ., have, a, great, time, flying, this]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ill, check, it, out, appreciate, the, response, regardless, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, service, agents, at, MCO, are, great, but, their, are, not, enough, of, them, working, right, now, !]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[agent, in, LAS, letting, 20, customers, know, they, ca, n't, help, them, rebook, delayed, flight, to, DEN, unfriendlyskies, http, :, //...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[I, 'm, trying, to, register, since, 12:00, do, n't, want, to, be, separated, from, my, brother, during, the, 15hours, flight, !, there'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[been, on, hold, over, an, hr, to, rebook, a, cancelled, flighted, flight, ., do, you, have, anyone, working, ?, ?, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[should, 've, been, on, one, of, your, flights, instead, ..., has, now, lost, our, bags, to, add, insult, to, injury]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[I, love, the, admiral, clubs, !, thanks, hey, can, you, follow, me, ?]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[is, your, company, motto, :, ``, I, do, not, care, !, !, !, ``, I, heard, several, times, from, your, employees, to, your, clients, in,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[I, was, abused, threatened, and, forced, to, travel, in, a, lower, cabine, (, last, seat, ), in, yesterday, 's, flight, from, houston, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[baggage, check, in, and, in, flight, crew, the, friendliest, ever, flight, 417, ogg, to, lax, !, !, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[is, not, being, responding, to, the, athletes, ., notcool, southwest]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[tired, of, sitting, on, a, delayed, 1702, again, and, again, computer, down]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[https, :, //t.co/jpd7NsGRT7, ., fyi, your, staff, are, telling, customers, to, try, other, carriers]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[too, long, for, 140, characters]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[it, 's, be, nice, to, take, a, flight, and, have, some, level, of, consistency, /, service, from, the, flight, attendants, ., quality, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[has, the, most, INCREDIBLE, customer, service, I, 've, ever, experienced, !, so, refreshing, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[these, are, great, fares, !, !, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[it, 's, only, because, I, 'm, wearing, truebluecolorstruebluecolorstruebluecolors, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[i, was, also, told, by, agents, my, issues, ``, are, n't, their, prob, ``, K, fine, ., I, get, it, ., but, have, some, compassion, 4, o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[yoga, it, is, ., TV, service, seems, to, be, down, though, :, -, (, I, look, forward, to, that, do, much, !]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[we, knew, it, was, not, going, to, leave, but, we, had, 6, hours, waiting, ..., that, was, the, hurry, ..]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[understand, waiting, 2, +, hours, for, callback, on, the, exec, plat, line, but, when, call, dropped, the, agent, could, at, least, try...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[..., these, changes, as, well, to, late, flightr, find, out, that, the, flight, I, was, scheduled, for, is, n't, ready]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[not, letting, me, DM, ., is, FSZ, 4YO]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[is, giving, me, false, hope, of, ever, getting, home]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[at, LAX, and, your, service, reps, just, hand, out, the, 800, number, to, call, ., so, that, 's, not, helpful, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[does, n't, care, nor, does, SNA, agent, jacquie, plitt, rude, gal, bad, attitude, !, had, 2, fly, 2, CABO, thx, http, :, //t.co/VbUxfPC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>[TY, !, but, their, site, says, ``, please, note, that, we, do, not, offer, a, print, subscription, service, for, american, way, ., ...,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>[“, united, :, we, will, follow, up, with, our, maintenance, team, ., thanks, for, the, tweet, ., ^, KP, ”, THANK, YOU, !, united, cares...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>[then, you, delete, my, return, ticket, to, europe, and, blame, me, for, the, now, show, ?, this, airline, is, a, joke, ., 3, times, thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>[ca, n't, believe, how, many, paying, customers, you, left, high, and, dry, with, no, reason, for, flight, cancelled, flightlations, mon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>[two, hour, wait, for, EXPs, as, I, sit, on, a, JFK, PHX, flight, because, US, computers, are, down, ., any, shot, at, an, LAX, flight, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>[thank, you, for, your, reply, ., my, frustration, is, that, spending, a, upgrade, just, puts, you, on, a, wait, list, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>[instead, of, making, seats, smaller, /, thinner, so, u, can, jam, more, people, on, a, flightu, should, concentrate, on, maint, &amp;, happ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>[ruined, our, honeymoon, by, causing, us, to, miss, our, international, flight, and, now, we, are, stranded, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4159</th>\n",
       "      <td>[seriously, fail, on, making, strangers, share, a, room]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4160</th>\n",
       "      <td>[i, will, be, writing, a, very, detailed, letter, to, you, all, about, this, experience, today, ., I, have, never, experienced, such, aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>[disappointed, that, u, didnt, honor, my, $, 100, credit, given, to, me, for, ur, mistakes, ., taking, my, business, elsewhere, ✌, ️, ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4162</th>\n",
       "      <td>[worst, company, ever, please, do, not, fly, with, them, I, repeat, please, do, not, fly, !, !, they, will, not, credit, you, if, you, '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>[flt, 648, from, buf, to, MCO, ., conffgkxvFGKXV, 5, ., please, also, look, into, reimburse, for, car, 5877019, 25COUNT, that, I, had, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4164</th>\n",
       "      <td>[capital, one, and, I, explained, the, false, fraud, alert, ., why, did, the, jet, blue, representative, issue, me, a, new, tkt, if, it,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>[love, them, !, always, get, the, best, deals, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>[was, far, less, painful, than, what, was, coming, from, avis, ., 💙]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>[I, am, now, following, you, and, see, that, I, am, not, the, only, one, experiencing, and, noticing, a, substantial, downgrade, in, ser...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>[delayed, my, flight, 3, times, before, cancelled, flighting, it, had, angry, and, rude, workers, and, are, now, providing, no, helpful,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>[thanks, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>[ok, thanks, ., safety, first, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>[how, do, I, change, flights, ., 3, hours, is, crazy, please, help]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>[thanks, yup, I, 'm, all, set, ., it, happens, ., SLC, ground, staff, were, prompt, helpful, and, courteous, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>[plane, breaks, unloads, the, passengers, told, to, re, enter, to, only, come, back, off, ., flight, cancelled, flightled, missed, weddi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>[kudos, to, the, crew, of, flight, 1050, to, GRR, for, making, a, very, special, memory, for, a, sweet, young, passenger, and, her, momm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>[stuck, in, houston, because, you, ca, n't, seem, to, get, a, plane, to, the, destination, on, time, in, perfect, flying, conditions, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>[flight, was, overbooked, !, was, offered, voucher, to, wait, &amp;, take, another, flight, ., gate, agent, switched, me, over, retracted, v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>[traveling, with, who, is, injured, ., gate, agent, in, chicago, was, awesome, helping, her, ., TY, roadwarrior]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>[that, 's, why, I, 'm, asking, for, exception, ., staffer, moving, last, minutewe, 're, a, nonprofit, &amp;, losing, $, 400we, just, want, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>[I, 'm, on, one, of, your, 757-300, between, JFK, and, LAX.When, r, u, upgrading, planes, ?, plane, has, no, screenslousy, seats, and, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>[6, hour, delay, ., supposed, to, land, at, 9pm, now, it, 's, 3am, ., still, not, boarding, ., epicfailepicfail]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4181 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             text  \\\n",
       "0                                                                                                      [got, it, ., thanks, the, quick, reply, .]   \n",
       "1             [I, ’, ve, filled, out, the, form, twice, ., no, email, ., I, have, a, lost, item, code, ., can, you, verify, it, was, received, ?]   \n",
       "2                                                                                [bluemanity, loves, this, ., have, a, great, time, flying, this]   \n",
       "3                                                                                 [ill, check, it, out, appreciate, the, response, regardless, .]   \n",
       "4                                     [you, service, agents, at, MCO, are, great, but, their, are, not, enough, of, them, working, right, now, !]   \n",
       "5     [agent, in, LAS, letting, 20, customers, know, they, ca, n't, help, them, rebook, delayed, flight, to, DEN, unfriendlyskies, http, :, //...   \n",
       "6     [I, 'm, trying, to, register, since, 12:00, do, n't, want, to, be, separated, from, my, brother, during, the, 15hours, flight, !, there'...   \n",
       "7                          [been, on, hold, over, an, hr, to, rebook, a, cancelled, flighted, flight, ., do, you, have, anyone, working, ?, ?, ?]   \n",
       "8                           [should, 've, been, on, one, of, your, flights, instead, ..., has, now, lost, our, bags, to, add, insult, to, injury]   \n",
       "9                                                                         [I, love, the, admiral, clubs, !, thanks, hey, can, you, follow, me, ?]   \n",
       "10    [is, your, company, motto, :, ``, I, do, not, care, !, !, !, ``, I, heard, several, times, from, your, employees, to, your, clients, in,...   \n",
       "11    [I, was, abused, threatened, and, forced, to, travel, in, a, lower, cabine, (, last, seat, ), in, yesterday, 's, flight, from, houston, ...   \n",
       "12                                        [baggage, check, in, and, in, flight, crew, the, friendliest, ever, flight, 417, ogg, to, lax, !, !, !]   \n",
       "13                                                                         [is, not, being, responding, to, the, athletes, ., notcool, southwest]   \n",
       "14                                                                  [tired, of, sitting, on, a, delayed, 1702, again, and, again, computer, down]   \n",
       "15                                          [https, :, //t.co/jpd7NsGRT7, ., fyi, your, staff, are, telling, customers, to, try, other, carriers]   \n",
       "16                                                                                                              [too, long, for, 140, characters]   \n",
       "17    [it, 's, be, nice, to, take, a, flight, and, have, some, level, of, consistency, /, service, from, the, flight, attendants, ., quality, ...   \n",
       "18                                               [has, the, most, INCREDIBLE, customer, service, I, 've, ever, experienced, !, so, refreshing, !]   \n",
       "19                                                                                                            [these, are, great, fares, !, !, !]   \n",
       "20                                                         [it, 's, only, because, I, 'm, wearing, truebluecolorstruebluecolorstruebluecolors, !]   \n",
       "21    [i, was, also, told, by, agents, my, issues, ``, are, n't, their, prob, ``, K, fine, ., I, get, it, ., but, have, some, compassion, 4, o...   \n",
       "22                                  [yoga, it, is, ., TV, service, seems, to, be, down, though, :, -, (, I, look, forward, to, that, do, much, !]   \n",
       "23                                    [we, knew, it, was, not, going, to, leave, but, we, had, 6, hours, waiting, ..., that, was, the, hurry, ..]   \n",
       "24    [understand, waiting, 2, +, hours, for, callback, on, the, exec, plat, line, but, when, call, dropped, the, agent, could, at, least, try...   \n",
       "25                       [..., these, changes, as, well, to, late, flightr, find, out, that, the, flight, I, was, scheduled, for, is, n't, ready]   \n",
       "26                                                                                                        [not, letting, me, DM, ., is, FSZ, 4YO]   \n",
       "27                                                                                         [is, giving, me, false, hope, of, ever, getting, home]   \n",
       "28                             [at, LAX, and, your, service, reps, just, hand, out, the, 800, number, to, call, ., so, that, 's, not, helpful, .]   \n",
       "29    [does, n't, care, nor, does, SNA, agent, jacquie, plitt, rude, gal, bad, attitude, !, had, 2, fly, 2, CABO, thx, http, :, //t.co/VbUxfPC...   \n",
       "...                                                                                                                                           ...   \n",
       "4151  [TY, !, but, their, site, says, ``, please, note, that, we, do, not, offer, a, print, subscription, service, for, american, way, ., ...,...   \n",
       "4152  [“, united, :, we, will, follow, up, with, our, maintenance, team, ., thanks, for, the, tweet, ., ^, KP, ”, THANK, YOU, !, united, cares...   \n",
       "4153  [then, you, delete, my, return, ticket, to, europe, and, blame, me, for, the, now, show, ?, this, airline, is, a, joke, ., 3, times, thi...   \n",
       "4154  [ca, n't, believe, how, many, paying, customers, you, left, high, and, dry, with, no, reason, for, flight, cancelled, flightlations, mon...   \n",
       "4155   [two, hour, wait, for, EXPs, as, I, sit, on, a, JFK, PHX, flight, because, US, computers, are, down, ., any, shot, at, an, LAX, flight, ?]   \n",
       "4156                    [thank, you, for, your, reply, ., my, frustration, is, that, spending, a, upgrade, just, puts, you, on, a, wait, list, .]   \n",
       "4157  [instead, of, making, seats, smaller, /, thinner, so, u, can, jam, more, people, on, a, flightu, should, concentrate, on, maint, &, happ...   \n",
       "4158                              [ruined, our, honeymoon, by, causing, us, to, miss, our, international, flight, and, now, we, are, stranded, .]   \n",
       "4159                                                                                     [seriously, fail, on, making, strangers, share, a, room]   \n",
       "4160  [i, will, be, writing, a, very, detailed, letter, to, you, all, about, this, experience, today, ., I, have, never, experienced, such, aw...   \n",
       "4161  [disappointed, that, u, didnt, honor, my, $, 100, credit, given, to, me, for, ur, mistakes, ., taking, my, business, elsewhere, ✌, ️, ou...   \n",
       "4162  [worst, company, ever, please, do, not, fly, with, them, I, repeat, please, do, not, fly, !, !, they, will, not, credit, you, if, you, '...   \n",
       "4163  [flt, 648, from, buf, to, MCO, ., conffgkxvFGKXV, 5, ., please, also, look, into, reimburse, for, car, 5877019, 25COUNT, that, I, had, t...   \n",
       "4164  [capital, one, and, I, explained, the, false, fraud, alert, ., why, did, the, jet, blue, representative, issue, me, a, new, tkt, if, it,...   \n",
       "4165                                                                                            [love, them, !, always, get, the, best, deals, !]   \n",
       "4166                                                                         [was, far, less, painful, than, what, was, coming, from, avis, ., 💙]   \n",
       "4167  [I, am, now, following, you, and, see, that, I, am, not, the, only, one, experiencing, and, noticing, a, substantial, downgrade, in, ser...   \n",
       "4168  [delayed, my, flight, 3, times, before, cancelled, flighting, it, had, angry, and, rude, workers, and, are, now, providing, no, helpful,...   \n",
       "4169                                                                                                                                  [thanks, !]   \n",
       "4170                                                                                                            [ok, thanks, ., safety, first, .]   \n",
       "4171                                                                          [how, do, I, change, flights, ., 3, hours, is, crazy, please, help]   \n",
       "4172                              [thanks, yup, I, 'm, all, set, ., it, happens, ., SLC, ground, staff, were, prompt, helpful, and, courteous, .]   \n",
       "4173  [plane, breaks, unloads, the, passengers, told, to, re, enter, to, only, come, back, off, ., flight, cancelled, flightled, missed, weddi...   \n",
       "4174  [kudos, to, the, crew, of, flight, 1050, to, GRR, for, making, a, very, special, memory, for, a, sweet, young, passenger, and, her, momm...   \n",
       "4175     [stuck, in, houston, because, you, ca, n't, seem, to, get, a, plane, to, the, destination, on, time, in, perfect, flying, conditions, .]   \n",
       "4176  [flight, was, overbooked, !, was, offered, voucher, to, wait, &, take, another, flight, ., gate, agent, switched, me, over, retracted, v...   \n",
       "4177                             [traveling, with, who, is, injured, ., gate, agent, in, chicago, was, awesome, helping, her, ., TY, roadwarrior]   \n",
       "4178  [that, 's, why, I, 'm, asking, for, exception, ., staffer, moving, last, minutewe, 're, a, nonprofit, &, losing, $, 400we, just, want, t...   \n",
       "4179  [I, 'm, on, one, of, your, 757-300, between, JFK, and, LAX.When, r, u, upgrading, planes, ?, plane, has, no, screenslousy, seats, and, t...   \n",
       "4180                             [6, hour, delay, ., supposed, to, land, at, 9pm, now, it, 's, 3am, ., still, not, boarding, ., epicfailepicfail]   \n",
       "\n",
       "      Sentiment  \n",
       "0             1  \n",
       "1             0  \n",
       "2             1  \n",
       "3             1  \n",
       "4             0  \n",
       "5             0  \n",
       "6             0  \n",
       "7             0  \n",
       "8             0  \n",
       "9             1  \n",
       "10            0  \n",
       "11            0  \n",
       "12            1  \n",
       "13            0  \n",
       "14            0  \n",
       "15            0  \n",
       "16            0  \n",
       "17            0  \n",
       "18            1  \n",
       "19            1  \n",
       "20            1  \n",
       "21            0  \n",
       "22            0  \n",
       "23            0  \n",
       "24            0  \n",
       "25            0  \n",
       "26            0  \n",
       "27            0  \n",
       "28            0  \n",
       "29            0  \n",
       "...         ...  \n",
       "4151          0  \n",
       "4152          1  \n",
       "4153          0  \n",
       "4154          0  \n",
       "4155          0  \n",
       "4156          0  \n",
       "4157          0  \n",
       "4158          0  \n",
       "4159          0  \n",
       "4160          0  \n",
       "4161          0  \n",
       "4162          0  \n",
       "4163          0  \n",
       "4164          0  \n",
       "4165          1  \n",
       "4166          1  \n",
       "4167          0  \n",
       "4168          0  \n",
       "4169          1  \n",
       "4170          1  \n",
       "4171          0  \n",
       "4172          1  \n",
       "4173          0  \n",
       "4174          1  \n",
       "4175          0  \n",
       "4176          0  \n",
       "4177          1  \n",
       "4178          0  \n",
       "4179          0  \n",
       "4180          0  \n",
       "\n",
       "[4181 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "stemtweets=[]\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "#ps= PorterStemmer()\n",
    "for x in df.text:\n",
    "    stemtweet=''\n",
    "    for word in x:\n",
    "        stemtweet=stemtweet+stemmer.stem(word)+' '\n",
    "    stemtweets.append(word_tokenize(stemtweet))\n",
    "df['stemmed']=stemtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[customer, service, at, its, best, !, rachel, S, ., took, great, care, of, us, at, the, PHX, airport, ., http, :, //t.co/HG7vEqhGHy]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[counter, agents, at, RDU, deserve, a, medal, ., thankyou]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[is, that, the, same, reliable, system, could, n't, find, my, info, and, then, said, it, refund, my, credit, card, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, got, a, new, reservation, for, tomorrow, ., thanks, !]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, hours, on, hold, and, no, answer, ., guess, the, synergy, of, a, merger, was, really, planned, out]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   text  \\\n",
       "0  [customer, service, at, its, best, !, rachel, S, ., took, great, care, of, us, at, the, PHX, airport, ., http, :, //t.co/HG7vEqhGHy]   \n",
       "1                                                                            [counter, agents, at, RDU, deserve, a, medal, ., thankyou]   \n",
       "2                 [is, that, the, same, reliable, system, could, n't, find, my, info, and, then, said, it, refund, my, credit, card, ?]   \n",
       "3                                                                            [i, got, a, new, reservation, for, tomorrow, ., thanks, !]   \n",
       "4                               [5, hours, on, hold, and, no, answer, ., guess, the, synergy, of, a, merger, was, really, planned, out]   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Finalize both the stemmed and unstemmed dataframes\n",
    "df_unstemmed = df.drop(['stemmed'], axis=1)\n",
    "df_unstemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[custom, servic, at, it, best, !, rachel, s, ., took, great, care, of, us, at, the, phx, airport, ., http, :, //t.co/hg7veqhghi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[counter, agent, at, rdu, deserv, a, medal, ., thankyou]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[is, that, the, same, reliabl, system, could, n't, find, my, info, and, then, said, it, refund, my, credit, card, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, got, a, new, reserv, for, tomorrow, ., thank, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[5, hour, on, hold, and, no, answer, ., guess, the, synergi, of, a, merger, was, realli, plan, out]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  \\\n",
       "0          1   \n",
       "1          1   \n",
       "2          0   \n",
       "3          1   \n",
       "4          0   \n",
       "\n",
       "                                                                                                                            stemmed  \n",
       "0  [custom, servic, at, it, best, !, rachel, s, ., took, great, care, of, us, at, the, phx, airport, ., http, :, //t.co/hg7veqhghi]  \n",
       "1                                                                          [counter, agent, at, rdu, deserv, a, medal, ., thankyou]  \n",
       "2              [is, that, the, same, reliabl, system, could, n't, find, my, info, and, then, said, it, refund, my, credit, card, ?]  \n",
       "3                                                                              [i, got, a, new, reserv, for, tomorrow, ., thank, !]  \n",
       "4                               [5, hour, on, hold, and, no, answer, ., guess, the, synergi, of, a, merger, was, realli, plan, out]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a df with stemmed text\n",
    "df_stemmed = df.drop(['text'], axis=1)\n",
    "df_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract text from df\n",
    "text = df['text']\n",
    "\n",
    "# initiate count vectorizer\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "vectorizer.fit(text)\n",
    "freqVocab = vectorizer.vocabulary_\n",
    "train_vector = vectorizer.transform(text)\n",
    "len(freqVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4181, 7674)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binaryVector = pd.Series(text).apply(pd.value_counts).fillna(0).astype(int)\n",
    "# binaryVector.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract text from df\n",
    "text = df['text']\n",
    "\n",
    "# initiate count vectorizer\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer2 = CountVectorizer(binary = True, analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)  \n",
    "vectorizer2.fit(text)\n",
    "binaryVocab = vectorizer2.vocabulary_\n",
    "train_binary_vector = vectorizer2.transform(text)\n",
    "len(binaryVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_binary_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency - No stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n",
      "(1181, 3)\n"
     ]
    }
   ],
   "source": [
    "# calculate the no. of negative sentiments\n",
    "neg =  df['Sentiment']==0\n",
    "NTrain = df[neg]\n",
    "print(neg_count.shape)\n",
    "NTrain.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# calculate the no. of positive sentiments\n",
    "pos = df['Sentiment']==1\n",
    "PTrain = df[pos]\n",
    "print(pos_count.shape)\n",
    "PTrain.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# calculate the prior for each class\n",
    "priors = []\n",
    "priors.append(PTrain.shape[0]/df.shape[0])\n",
    "priors.append(NTrain.shape[0]/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform pos and neg tweets into seprate vectors\n",
    "train_pos_vector1 = vectorizer.transform(PTrain['text'])\n",
    "train_pos_vector1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7674)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_vector1 = vectorizer.transform(NTrain['text'])\n",
    "train_neg_vector1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pos = train_pos_vector1.sum(axis = 0)\n",
    "sum_neg = train_neg_vector1.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdoc = pd.DataFrame(index = list(set(freqVocab.keys())), columns = ['pos', 'neg'])\n",
    "#bigdoc.index = list(set(freqVocab.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in freqVocab.keys():\n",
    "    index = freqVocab.get(word)\n",
    "    bigdoc.at[word, 'pos'] = sum_pos[:, index]\n",
    "    bigdoc.at[word, 'neg'] = sum_neg[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate count vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Naivebayes(data,category,vector,bigvec):\n",
    "    priors=[]\n",
    "    for cat in category:\n",
    "        ndoc= len(data)\n",
    "        nc= len(data['Sentiment']== cat)\n",
    "        prior = nc/ndoc\n",
    "        print(prior)\n",
    "    \n",
    "        priors.append(prior)\n",
    "        vocab= vector\n",
    "        is_cat =  data['Sentiment']== cat\n",
    "        bigdoc= data[is_cat]\n",
    "        bigdocvec= bigvec\n",
    "        case_list= []\n",
    "        for word2 in bigdocvec:\n",
    "            for word in vocab:\n",
    "                if(word==word2):\n",
    "                    logprob= math.log(bigdocvec[word2]+1/(sum(vocab.values())+1))\n",
    "            case= {word2:logprob}\n",
    "            case_list.append(case)\n",
    "            \n",
    "        return [case_list,prior]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "result = Naivebayes(df, [0,1], freqVector,bigdocVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
